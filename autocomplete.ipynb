{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de6e8bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2243c416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\DELL'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ad3f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"F:/Auto completion/AUTO_COMPLETE_STREAMLIT-master/DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc9a2262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\Auto completion\\\\AUTO_COMPLETE_STREAMLIT-master\\\\DATA'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9228a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "with  open('data.txt','r',encoding='utf-8') as f:\n",
    "    file=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c619cfc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148201"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c73817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   GOOD WILL HUNTING\n",
      "           by\n",
      "Matt Damon & Ben Affleck\n",
      "\n",
      "FADE IN:\n",
      "EXT. SOUTH BOSTON ST. PATRICK'\n"
     ]
    }
   ],
   "source": [
    "print(file[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "318d4f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"GOOD WILL HUNTING by Matt Damon & Ben Affleck FADE IN: EXT. SOUTH BOSTON ST. PATRICK'S DAY PARADE DAY CUT TO: INT. L STREET BAR & GRILLE, SOUTH BOSTON EVENING The bar is dirty, more than a little run down. If there is ever a cook on duty, he's not here now. As we pan across several empty tables, we can almost smell the odor of last nights beer and crushed pretzels on the floor. CHUCKIE Oh my God, I got the most fucked up thing I been meanin' to tell you. As the camera rises, we find FOUR YOUNG M\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store files in list\n",
    "#lines=[]\n",
    "#for i in file:\n",
    "    #lines.append(i)\n",
    "lines = file.split('\\n')\n",
    "#convert list to string\n",
    "data=' '.join(lines)\n",
    "#replace unnecessary stuff with space\n",
    "data=data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('\"', ' ').replace('--', ' ')  #new line, carriage return, unicode character, opening and closing double quotes with spaces\n",
    "#remove unnecessary spaces\n",
    "data=data.split()\n",
    "data=' '.join(data)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5152ad24",
   "metadata": {},
   "source": [
    "## Applying Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2afa186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96, 2, 645, 90, 1527, 1528, 1529, 1530, 1024, 12, 77, 225, 163, 1025, 1531]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "#saving tokenizer for predict function\n",
    "pickle.dump(tokenizer,open('token.pkl','wb'))\n",
    "\n",
    "sequence_data=tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26592074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23607"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e63e4c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of sequences are: 23604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  96,    2,  645,   90],\n",
       "       [   2,  645,   90, 1527],\n",
       "       [ 645,   90, 1527, 1528],\n",
       "       [  90, 1527, 1528, 1529],\n",
       "       [1527, 1528, 1529, 1530],\n",
       "       [1528, 1529, 1530, 1024],\n",
       "       [1529, 1530, 1024,   12],\n",
       "       [1530, 1024,   12,   77],\n",
       "       [1024,   12,   77,  225],\n",
       "       [  12,   77,  225,  163]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences=[]\n",
    "for i in range(3,len(sequence_data)):\n",
    "    words=sequence_data[i-3:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"the length of sequences are:\",len(sequences))\n",
    "sequences=np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3132b2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "y=[]\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0:3])\n",
    "    y.append(i[3])\n",
    "    \n",
    "X=np.array(X)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aaae55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  [[  96    2  645]\n",
      " [   2  645   90]\n",
      " [ 645   90 1527]\n",
      " [  90 1527 1528]\n",
      " [1527 1528 1529]\n",
      " [1528 1529 1530]\n",
      " [1529 1530 1024]\n",
      " [1530 1024   12]\n",
      " [1024   12   77]\n",
      " [  12   77  225]]\n",
      "Response:  [  90 1527 1528 1529 1530 1024   12   77  225  163]\n"
     ]
    }
   ],
   "source": [
    "print(\"Data: \",X[:10])\n",
    "print(\"Response: \",y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26c71aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3293\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(tokenizer.word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9fec123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=to_categorical(y,num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f2dcd0",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c2c0627",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size,10,input_length=3))\n",
    "model.add(LSTM(1000,return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000,activation=\"relu\"))\n",
    "model.add(Dense(vocab_size,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "441ac9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 3, 10)             32930     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 3, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3293)              3296293   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,378,223\n",
      "Trainable params: 16,378,223\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5de5b302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 6.5654\n",
      "Epoch 1: loss improved from inf to 6.56543, saving model to next_words.h5\n",
      "369/369 [==============================] - 73s 189ms/step - loss: 6.5654\n",
      "Epoch 2/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 6.1344\n",
      "Epoch 2: loss improved from 6.56543 to 6.13436, saving model to next_words.h5\n",
      "369/369 [==============================] - 69s 188ms/step - loss: 6.1344\n",
      "Epoch 3/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 5.8316\n",
      "Epoch 3: loss improved from 6.13436 to 5.83163, saving model to next_words.h5\n",
      "369/369 [==============================] - 69s 187ms/step - loss: 5.8316\n",
      "Epoch 4/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 5.5672\n",
      "Epoch 4: loss improved from 5.83163 to 5.56716, saving model to next_words.h5\n",
      "369/369 [==============================] - 69s 187ms/step - loss: 5.5672\n",
      "Epoch 5/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 5.3183\n",
      "Epoch 5: loss improved from 5.56716 to 5.31827, saving model to next_words.h5\n",
      "369/369 [==============================] - 69s 187ms/step - loss: 5.3183\n",
      "Epoch 6/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 5.0812\n",
      "Epoch 6: loss improved from 5.31827 to 5.08121, saving model to next_words.h5\n",
      "369/369 [==============================] - 69s 187ms/step - loss: 5.0812\n",
      "Epoch 7/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 4.8473\n",
      "Epoch 7: loss improved from 5.08121 to 4.84734, saving model to next_words.h5\n",
      "369/369 [==============================] - 69s 187ms/step - loss: 4.8473\n",
      "Epoch 8/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 4.6115\n",
      "Epoch 8: loss improved from 4.84734 to 4.61145, saving model to next_words.h5\n",
      "369/369 [==============================] - 69s 187ms/step - loss: 4.6115\n",
      "Epoch 9/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 4.3670\n",
      "Epoch 9: loss improved from 4.61145 to 4.36695, saving model to next_words.h5\n",
      "369/369 [==============================] - 69s 187ms/step - loss: 4.3670\n",
      "Epoch 10/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 4.1055\n",
      "Epoch 10: loss improved from 4.36695 to 4.10553, saving model to next_words.h5\n",
      "369/369 [==============================] - 69s 187ms/step - loss: 4.1055\n",
      "Epoch 11/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 3.8156\n",
      "Epoch 11: loss improved from 4.10553 to 3.81556, saving model to next_words.h5\n",
      "369/369 [==============================] - 69s 187ms/step - loss: 3.8156\n",
      "Epoch 12/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 3.5127\n",
      "Epoch 12: loss improved from 3.81556 to 3.51267, saving model to next_words.h5\n",
      "369/369 [==============================] - 70s 190ms/step - loss: 3.5127\n",
      "Epoch 13/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 3.2115\n",
      "Epoch 13: loss improved from 3.51267 to 3.21154, saving model to next_words.h5\n",
      "369/369 [==============================] - 70s 189ms/step - loss: 3.2115\n",
      "Epoch 14/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 2.9083\n",
      "Epoch 14: loss improved from 3.21154 to 2.90834, saving model to next_words.h5\n",
      "369/369 [==============================] - 70s 190ms/step - loss: 2.9083\n",
      "Epoch 15/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 2.6307\n",
      "Epoch 15: loss improved from 2.90834 to 2.63074, saving model to next_words.h5\n",
      "369/369 [==============================] - 69s 188ms/step - loss: 2.6307\n",
      "Epoch 16/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 2.3509\n",
      "Epoch 16: loss improved from 2.63074 to 2.35087, saving model to next_words.h5\n",
      "369/369 [==============================] - 71s 193ms/step - loss: 2.3509\n",
      "Epoch 17/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 2.1046\n",
      "Epoch 17: loss improved from 2.35087 to 2.10460, saving model to next_words.h5\n",
      "369/369 [==============================] - 71s 194ms/step - loss: 2.1046\n",
      "Epoch 18/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 1.8622\n",
      "Epoch 18: loss improved from 2.10460 to 1.86218, saving model to next_words.h5\n",
      "369/369 [==============================] - 72s 194ms/step - loss: 1.8622\n",
      "Epoch 19/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 1.6324\n",
      "Epoch 19: loss improved from 1.86218 to 1.63236, saving model to next_words.h5\n",
      "369/369 [==============================] - 71s 192ms/step - loss: 1.6324\n",
      "Epoch 20/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 1.4356\n",
      "Epoch 20: loss improved from 1.63236 to 1.43560, saving model to next_words.h5\n",
      "369/369 [==============================] - 71s 192ms/step - loss: 1.4356\n",
      "Epoch 21/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 1.2457\n",
      "Epoch 21: loss improved from 1.43560 to 1.24573, saving model to next_words.h5\n",
      "369/369 [==============================] - 72s 194ms/step - loss: 1.2457\n",
      "Epoch 22/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 1.0679\n",
      "Epoch 22: loss improved from 1.24573 to 1.06791, saving model to next_words.h5\n",
      "369/369 [==============================] - 71s 193ms/step - loss: 1.0679\n",
      "Epoch 23/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.9204\n",
      "Epoch 23: loss improved from 1.06791 to 0.92036, saving model to next_words.h5\n",
      "369/369 [==============================] - 71s 193ms/step - loss: 0.9204\n",
      "Epoch 24/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.7977\n",
      "Epoch 24: loss improved from 0.92036 to 0.79775, saving model to next_words.h5\n",
      "369/369 [==============================] - 71s 192ms/step - loss: 0.7977\n",
      "Epoch 25/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.6975\n",
      "Epoch 25: loss improved from 0.79775 to 0.69751, saving model to next_words.h5\n",
      "369/369 [==============================] - 72s 197ms/step - loss: 0.6975\n",
      "Epoch 26/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.6212\n",
      "Epoch 26: loss improved from 0.69751 to 0.62121, saving model to next_words.h5\n",
      "369/369 [==============================] - 72s 195ms/step - loss: 0.6212\n",
      "Epoch 27/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.5494\n",
      "Epoch 27: loss improved from 0.62121 to 0.54943, saving model to next_words.h5\n",
      "369/369 [==============================] - 72s 195ms/step - loss: 0.5494\n",
      "Epoch 28/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.5080\n",
      "Epoch 28: loss improved from 0.54943 to 0.50802, saving model to next_words.h5\n",
      "369/369 [==============================] - 71s 193ms/step - loss: 0.5080\n",
      "Epoch 29/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.4626\n",
      "Epoch 29: loss improved from 0.50802 to 0.46261, saving model to next_words.h5\n",
      "369/369 [==============================] - 72s 196ms/step - loss: 0.4626\n",
      "Epoch 30/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.4369\n",
      "Epoch 30: loss improved from 0.46261 to 0.43687, saving model to next_words.h5\n",
      "369/369 [==============================] - 71s 192ms/step - loss: 0.4369\n",
      "Epoch 31/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.4127\n",
      "Epoch 31: loss improved from 0.43687 to 0.41271, saving model to next_words.h5\n",
      "369/369 [==============================] - 72s 196ms/step - loss: 0.4127\n",
      "Epoch 32/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.3995\n",
      "Epoch 32: loss improved from 0.41271 to 0.39950, saving model to next_words.h5\n",
      "369/369 [==============================] - 73s 197ms/step - loss: 0.3995\n",
      "Epoch 33/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.3884\n",
      "Epoch 33: loss improved from 0.39950 to 0.38836, saving model to next_words.h5\n",
      "369/369 [==============================] - 72s 195ms/step - loss: 0.3884\n",
      "Epoch 34/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.3785\n",
      "Epoch 34: loss improved from 0.38836 to 0.37851, saving model to next_words.h5\n",
      "369/369 [==============================] - 71s 193ms/step - loss: 0.3785\n",
      "Epoch 35/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.3546\n",
      "Epoch 35: loss improved from 0.37851 to 0.35459, saving model to next_words.h5\n",
      "369/369 [==============================] - 70s 191ms/step - loss: 0.3546\n",
      "Epoch 36/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.3351\n",
      "Epoch 36: loss improved from 0.35459 to 0.33513, saving model to next_words.h5\n",
      "369/369 [==============================] - 71s 192ms/step - loss: 0.3351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.3256\n",
      "Epoch 37: loss improved from 0.33513 to 0.32556, saving model to next_words.h5\n",
      "369/369 [==============================] - 70s 191ms/step - loss: 0.3256\n",
      "Epoch 38/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.3294\n",
      "Epoch 38: loss did not improve from 0.32556\n",
      "369/369 [==============================] - 71s 192ms/step - loss: 0.3294\n",
      "Epoch 39/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.3217\n",
      "Epoch 39: loss improved from 0.32556 to 0.32169, saving model to next_words.h5\n",
      "369/369 [==============================] - 71s 192ms/step - loss: 0.3217\n",
      "Epoch 40/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.3136\n",
      "Epoch 40: loss improved from 0.32169 to 0.31358, saving model to next_words.h5\n",
      "369/369 [==============================] - 70s 191ms/step - loss: 0.3136\n",
      "Epoch 41/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.3215\n",
      "Epoch 41: loss did not improve from 0.31358\n",
      "369/369 [==============================] - 70s 190ms/step - loss: 0.3215\n",
      "Epoch 42/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.3058\n",
      "Epoch 42: loss improved from 0.31358 to 0.30582, saving model to next_words.h5\n",
      "369/369 [==============================] - 73s 197ms/step - loss: 0.3058\n",
      "Epoch 43/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2882\n",
      "Epoch 43: loss improved from 0.30582 to 0.28821, saving model to next_words.h5\n",
      "369/369 [==============================] - 73s 197ms/step - loss: 0.2882\n",
      "Epoch 44/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2952\n",
      "Epoch 44: loss did not improve from 0.28821\n",
      "369/369 [==============================] - 72s 194ms/step - loss: 0.2952\n",
      "Epoch 45/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2875\n",
      "Epoch 45: loss improved from 0.28821 to 0.28754, saving model to next_words.h5\n",
      "369/369 [==============================] - 72s 195ms/step - loss: 0.2875\n",
      "Epoch 46/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2840\n",
      "Epoch 46: loss improved from 0.28754 to 0.28399, saving model to next_words.h5\n",
      "369/369 [==============================] - 72s 194ms/step - loss: 0.2840\n",
      "Epoch 47/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2836\n",
      "Epoch 47: loss improved from 0.28399 to 0.28364, saving model to next_words.h5\n",
      "369/369 [==============================] - 72s 194ms/step - loss: 0.2836\n",
      "Epoch 48/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2712\n",
      "Epoch 48: loss improved from 0.28364 to 0.27120, saving model to next_words.h5\n",
      "369/369 [==============================] - 72s 196ms/step - loss: 0.2712\n",
      "Epoch 49/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2669\n",
      "Epoch 49: loss improved from 0.27120 to 0.26687, saving model to next_words.h5\n",
      "369/369 [==============================] - 71s 193ms/step - loss: 0.2669\n",
      "Epoch 50/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2599\n",
      "Epoch 50: loss improved from 0.26687 to 0.25989, saving model to next_words.h5\n",
      "369/369 [==============================] - 71s 193ms/step - loss: 0.2599\n",
      "Epoch 51/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2648\n",
      "Epoch 51: loss did not improve from 0.25989\n",
      "369/369 [==============================] - 71s 193ms/step - loss: 0.2648\n",
      "Epoch 52/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2538\n",
      "Epoch 52: loss improved from 0.25989 to 0.25380, saving model to next_words.h5\n",
      "369/369 [==============================] - 72s 194ms/step - loss: 0.2538\n",
      "Epoch 53/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2453\n",
      "Epoch 53: loss improved from 0.25380 to 0.24531, saving model to next_words.h5\n",
      "369/369 [==============================] - 71s 194ms/step - loss: 0.2453\n",
      "Epoch 54/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2397\n",
      "Epoch 54: loss improved from 0.24531 to 0.23968, saving model to next_words.h5\n",
      "369/369 [==============================] - 72s 195ms/step - loss: 0.2397\n",
      "Epoch 55/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2403\n",
      "Epoch 55: loss did not improve from 0.23968\n",
      "369/369 [==============================] - 71s 192ms/step - loss: 0.2403\n",
      "Epoch 56/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2613\n",
      "Epoch 56: loss did not improve from 0.23968\n",
      "369/369 [==============================] - 71s 194ms/step - loss: 0.2613\n",
      "Epoch 57/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2695\n",
      "Epoch 57: loss did not improve from 0.23968\n",
      "369/369 [==============================] - 71s 192ms/step - loss: 0.2695\n",
      "Epoch 58/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2589\n",
      "Epoch 58: loss did not improve from 0.23968\n",
      "369/369 [==============================] - 71s 193ms/step - loss: 0.2589\n",
      "Epoch 59/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2419\n",
      "Epoch 59: loss did not improve from 0.23968\n",
      "369/369 [==============================] - 72s 194ms/step - loss: 0.2419\n",
      "Epoch 60/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2279\n",
      "Epoch 60: loss improved from 0.23968 to 0.22793, saving model to next_words.h5\n",
      "369/369 [==============================] - 72s 195ms/step - loss: 0.2279\n",
      "Epoch 61/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2149\n",
      "Epoch 61: loss improved from 0.22793 to 0.21488, saving model to next_words.h5\n",
      "369/369 [==============================] - 72s 194ms/step - loss: 0.2149\n",
      "Epoch 62/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2137\n",
      "Epoch 62: loss improved from 0.21488 to 0.21373, saving model to next_words.h5\n",
      "369/369 [==============================] - 71s 193ms/step - loss: 0.2137\n",
      "Epoch 63/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2071\n",
      "Epoch 63: loss improved from 0.21373 to 0.20709, saving model to next_words.h5\n",
      "369/369 [==============================] - 72s 195ms/step - loss: 0.2071\n",
      "Epoch 64/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2063\n",
      "Epoch 64: loss improved from 0.20709 to 0.20635, saving model to next_words.h5\n",
      "369/369 [==============================] - 72s 194ms/step - loss: 0.2063\n",
      "Epoch 65/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2144\n",
      "Epoch 65: loss did not improve from 0.20635\n",
      "369/369 [==============================] - 71s 193ms/step - loss: 0.2144\n",
      "Epoch 66/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2419\n",
      "Epoch 66: loss did not improve from 0.20635\n",
      "369/369 [==============================] - 71s 193ms/step - loss: 0.2419\n",
      "Epoch 67/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2828\n",
      "Epoch 67: loss did not improve from 0.20635\n",
      "369/369 [==============================] - 72s 194ms/step - loss: 0.2828\n",
      "Epoch 68/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2429\n",
      "Epoch 68: loss did not improve from 0.20635\n",
      "369/369 [==============================] - 72s 194ms/step - loss: 0.2429\n",
      "Epoch 69/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.2144\n",
      "Epoch 69: loss did not improve from 0.20635\n",
      "369/369 [==============================] - 72s 195ms/step - loss: 0.2144\n",
      "Epoch 70/70\n",
      "369/369 [==============================] - ETA: 0s - loss: 0.1996\n",
      "Epoch 70: loss improved from 0.20635 to 0.19957, saving model to next_words.h5\n",
      "369/369 [==============================] - 71s 193ms/step - loss: 0.1996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27897fec850>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint=ModelCheckpoint(\"next_words.h5\",monitor=\"loss\",verbose=1,save_best_only=True)\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=Adam(learning_rate=0.001))\n",
    "model.fit(X,y,epochs=70,batch_size=64,callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d3d7b",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fca777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf287613",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model and tokenizer\n",
    "model=load_model('next_words.h5')\n",
    "tokenizer=pickle.load(open('token.pkl','rb'))\n",
    "\n",
    "def Predict_Next_Words(model,tokenizer,text):\n",
    "    sequence=tokenizer.texts_to_sequences([text])\n",
    "    sequence=np.array(sequence)\n",
    "    preds=np.argmax(model.predict(sequence))\n",
    "    predicted_word=\"\"\n",
    "\n",
    "    for key,value in tokenizer.word_index.items():\n",
    "        if value==preds:\n",
    "            predicted_word=key\n",
    "            break\n",
    "    print(predicted_word)\n",
    "    return predicted_word\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "331f6997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter you line: As the camera\n",
      "['As', 'the', 'camera']\n",
      "1/1 [==============================] - 1s 650ms/step\n",
      "rises\n",
      "Enter you line: \n",
      "['']\n",
      "Error occured: \n",
      "Enter you line: \n",
      "['']\n",
      "Error occured: \n",
      "Enter you line: hi\n",
      "['hi']\n",
      "1/1 [==============================] - 1s 571ms/step\n",
      "i\n",
      "Enter you line: 0\n",
      "Execution completed...\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    text=input(\"Enter you line: \")\n",
    "    \n",
    "    if text==\"0\":\n",
    "        print(\"Execution completed...\")\n",
    "        break\n",
    "    else:\n",
    "        try:\n",
    "            text=text.split(\" \")\n",
    "            text=text[-3:]\n",
    "            print(text)\n",
    "            Predict_Next_Words(model,tokenizer,text)\n",
    "        except Exception as e:\n",
    "            print(\"Error occured: \")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8733f8a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
